"""
AI è°ƒåº¦ç³»ç»Ÿ
æ™ºèƒ½ç®¡ç† AI åˆ†æä»»åŠ¡çš„è°ƒåº¦ã€ä¼˜å…ˆçº§å’Œç¼“å­˜
"""
import asyncio
import json
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from enum import Enum
from loguru import logger

from app.database import db
from .ai_analyzer import ai_analyzer
from .deepseek_service import deepseek_service


class Priority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    HIGH = 1    # é«˜åˆ†é’±åŒ…ã€å¼‚å¸¸é’±åŒ…
    MEDIUM = 2  # æ´»è·ƒé’±åŒ…
    LOW = 3     # æ™®é€šé’±åŒ…


class AnalysisTask:
    """åˆ†æä»»åŠ¡"""
    
    def __init__(
        self,
        wallet_address: str,
        analysis_types: List[str],
        priority: Priority = Priority.MEDIUM,
        force: bool = False
    ):
        self.wallet_address = wallet_address
        self.analysis_types = analysis_types
        self.priority = priority
        self.force = force  # æ˜¯å¦å¼ºåˆ¶åˆ†æï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        self.created_at = datetime.now()
    
    def __lt__(self, other):
        """ç”¨äºä¼˜å…ˆçº§é˜Ÿåˆ—æ’åº"""
        return self.priority.value < other.priority.value


class AIScheduler:
    """AI è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.queue = asyncio.PriorityQueue()
        self.running = False
        self.current_task = None
        self.task_history = []
        
        # ç¼“å­˜é…ç½®
        self.cache_ttl = {
            'style': 86400 * 7,      # 7å¤©
            'strategy': 86400 * 7,   # 7å¤©
            'risk': 86400 * 3,       # 3å¤©
            'market': 3600           # 1å°æ—¶
        }
    
    async def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.running:
            logger.warning("AI è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.running = True
        logger.info("ğŸ¤– AI è°ƒåº¦å™¨å¯åŠ¨")
        
        # å¯åŠ¨åå°ä»»åŠ¡å¤„ç†
        asyncio.create_task(self._process_queue())
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.running = False
        logger.info("AI è°ƒåº¦å™¨åœæ­¢")
    
    async def schedule_analysis(
        self,
        wallet_address: str,
        analysis_types: Optional[List[str]] = None,
        priority: Priority = Priority.MEDIUM,
        force: bool = False
    ) -> Dict[str, Any]:
        """
        è°ƒåº¦åˆ†æä»»åŠ¡
        
        Args:
            wallet_address: é’±åŒ…åœ°å€
            analysis_types: åˆ†æç±»å‹åˆ—è¡¨
            priority: ä¼˜å…ˆçº§
            force: æ˜¯å¦å¼ºåˆ¶åˆ†æ
            
        Returns:
            ä»»åŠ¡ä¿¡æ¯
        """
        if analysis_types is None:
            analysis_types = ['style', 'strategy', 'risk']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ
        if not force:
            needs_analysis = []
            for analysis_type in analysis_types:
                if self._should_analyze(wallet_address, analysis_type):
                    needs_analysis.append(analysis_type)
            
            if not needs_analysis:
                logger.info(f"é’±åŒ… {wallet_address} çš„åˆ†æç»“æœä»æœ‰æ•ˆï¼Œä½¿ç”¨ç¼“å­˜")
                return {
                    'status': 'cached',
                    'message': 'ä½¿ç”¨ç¼“å­˜ç»“æœ',
                    'wallet_address': wallet_address
                }
            
            analysis_types = needs_analysis
        
        # åˆ›å»ºä»»åŠ¡
        task = AnalysisTask(wallet_address, analysis_types, priority, force)
        
        # åŠ å…¥é˜Ÿåˆ—
        await self.queue.put((priority.value, task))
        
        logger.info(
            f"å·²è°ƒåº¦åˆ†æä»»åŠ¡: {wallet_address}, "
            f"ç±»å‹: {analysis_types}, ä¼˜å…ˆçº§: {priority.name}"
        )
        
        return {
            'status': 'scheduled',
            'message': 'ä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—',
            'wallet_address': wallet_address,
            'analysis_types': analysis_types,
            'priority': priority.name,
            'queue_size': self.queue.qsize()
        }
    
    async def batch_schedule(
        self,
        wallet_addresses: List[str],
        analysis_types: Optional[List[str]] = None,
        priority: Priority = Priority.MEDIUM
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡è°ƒåº¦åˆ†æä»»åŠ¡
        
        Args:
            wallet_addresses: é’±åŒ…åœ°å€åˆ—è¡¨
            analysis_types: åˆ†æç±»å‹åˆ—è¡¨
            priority: ä¼˜å…ˆçº§
            
        Returns:
            æ‰¹é‡ä»»åŠ¡ä¿¡æ¯
        """
        scheduled = []
        cached = []
        
        for address in wallet_addresses:
            result = await self.schedule_analysis(
                address,
                analysis_types,
                priority,
                force=False
            )
            
            if result['status'] == 'scheduled':
                scheduled.append(address)
            else:
                cached.append(address)
        
        return {
            'total': len(wallet_addresses),
            'scheduled': len(scheduled),
            'cached': len(cached),
            'queue_size': self.queue.qsize()
        }
    
    async def _process_queue(self):
        """å¤„ç†ä»»åŠ¡é˜Ÿåˆ—"""
        logger.info("AI ä»»åŠ¡å¤„ç†å™¨å¯åŠ¨")
        
        while self.running:
            try:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨
                if not deepseek_service.is_enabled():
                    await asyncio.sleep(60)
                    continue
                
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶ 1 ç§’ï¼‰
                try:
                    priority, task = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue
                
                # å¤„ç†ä»»åŠ¡
                self.current_task = task
                await self._execute_task(task)
                self.current_task = None
                
                # è®°å½•å†å²
                self.task_history.append({
                    'wallet_address': task.wallet_address,
                    'analysis_types': task.analysis_types,
                    'priority': task.priority.name,
                    'completed_at': datetime.now().isoformat()
                })
                
                # é™åˆ¶å†å²è®°å½•æ•°é‡
                if len(self.task_history) > 1000:
                    self.task_history = self.task_history[-1000:]
                
                # ä»»åŠ¡é—´éš”ï¼ˆé¿å…è¿‡äºé¢‘ç¹ï¼‰
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥: {e}")
                await asyncio.sleep(5)
        
        logger.info("AI ä»»åŠ¡å¤„ç†å™¨åœæ­¢")
    
    async def _execute_task(self, task: AnalysisTask):
        """æ‰§è¡Œåˆ†æä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.wallet_address}")
            
            # è·å–é’±åŒ…æ•°æ®
            wallet_data = self._get_wallet_data(task.wallet_address)
            
            if not wallet_data:
                logger.warning(f"é’±åŒ…æ•°æ®ä¸å­˜åœ¨: {task.wallet_address}")
                return
            
            # æ‰§è¡Œåˆ†æ
            results = {}
            
            for analysis_type in task.analysis_types:
                try:
                    if analysis_type == 'style':
                        result = await ai_analyzer.analyze_trading_style(wallet_data)
                    elif analysis_type == 'strategy':
                        result = await ai_analyzer.identify_strategy(wallet_data)
                    elif analysis_type == 'risk':
                        result = await ai_analyzer.assess_risk(wallet_data)
                    else:
                        logger.warning(f"æœªçŸ¥çš„åˆ†æç±»å‹: {analysis_type}")
                        continue
                    
                    results[analysis_type] = result
                    
                    # ç¼“å­˜ç»“æœ
                    self._cache_result(task.wallet_address, analysis_type, result)
                    
                    logger.info(f"å®Œæˆ {analysis_type} åˆ†æ: {task.wallet_address}")
                    
                except Exception as e:
                    logger.error(f"{analysis_type} åˆ†æå¤±è´¥: {e}")
                    results[analysis_type] = {'error': str(e)}
            
            # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
            self._save_analysis(task.wallet_address, results)
            
            logger.info(f"ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task.wallet_address}")
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def _get_wallet_data(self, wallet_address: str) -> Optional[Dict[str, Any]]:
        """è·å–é’±åŒ…æ•°æ®"""
        try:
            # ä»æ•°æ®åº“è·å–é’±åŒ…ä¿¡æ¯
            wallet = db.fetch_one(
                "SELECT * FROM wallets WHERE address = ?",
                (wallet_address,)
            )
            
            if not wallet:
                return None
            
            # è·å–äº¤æ˜“æ•°æ®
            trades = db.fetch_all(
                "SELECT * FROM trades WHERE wallet_address = ? ORDER BY timestamp DESC LIMIT 100",
                (wallet_address,)
            )
            
            # æ„å»ºåˆ†ææ•°æ®
            wallet_data = {
                'address': wallet_address,
                'score': wallet.get('score', 0),
                'metrics': json.loads(wallet.get('metrics', '{}')),
                'tags': json.loads(wallet.get('tags', '[]')),
                'trades': [dict(trade) for trade in trades] if trades else []
            }
            
            return wallet_data
            
        except Exception as e:
            logger.error(f"è·å–é’±åŒ…æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _should_analyze(self, wallet_address: str, analysis_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache = db.fetch_one("""
                SELECT * FROM ai_analysis_cache
                WHERE wallet_address = ? AND analysis_type = ?
            """, (wallet_address, analysis_type))
            
            if not cache:
                return True
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            expires_at = datetime.fromisoformat(cache['expires_at'])
            if datetime.now() >= expires_at:
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥åˆ†æéœ€æ±‚å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶æ‰§è¡Œåˆ†æ
    
    def _cache_result(
        self,
        wallet_address: str,
        analysis_type: str,
        result: Dict[str, Any]
    ):
        """ç¼“å­˜åˆ†æç»“æœ"""
        try:
            ttl = self.cache_ttl.get(analysis_type, 86400)
            expires_at = datetime.now() + timedelta(seconds=ttl)
            
            db.execute("""
                INSERT OR REPLACE INTO ai_analysis_cache
                (wallet_address, analysis_type, result, created_at, expires_at)
                VALUES (?, ?, ?, ?, ?)
            """, (
                wallet_address,
                analysis_type,
                json.dumps(result, ensure_ascii=False),
                datetime.now().isoformat(),
                expires_at.isoformat()
            ))
            
            logger.debug(f"ç¼“å­˜åˆ†æç»“æœ: {wallet_address} - {analysis_type}")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜ç»“æœå¤±è´¥: {e}")
    
    def _save_analysis(self, wallet_address: str, results: Dict[str, Any]):
        """ä¿å­˜ç»¼åˆåˆ†æç»“æœ"""
        try:
            # æ›´æ–°é’±åŒ…çš„ AI æ ‡ç­¾
            ai_tags = self._extract_ai_tags(results)
            
            if ai_tags:
                # è·å–ç°æœ‰æ ‡ç­¾
                wallet = db.fetch_one(
                    "SELECT tags FROM wallets WHERE address = ?",
                    (wallet_address,)
                )
                
                if wallet:
                    existing_tags = json.loads(wallet.get('tags', '[]'))
                    
                    # ç§»é™¤æ—§çš„ AI æ ‡ç­¾
                    existing_tags = [
                        tag for tag in existing_tags
                        if not tag.startswith('AI:')
                    ]
                    
                    # æ·»åŠ æ–°çš„ AI æ ‡ç­¾
                    existing_tags.extend(ai_tags)
                    
                    # æ›´æ–°æ•°æ®åº“
                    db.execute(
                        "UPDATE wallets SET tags = ?, updated_at = ? WHERE address = ?",
                        (json.dumps(existing_tags, ensure_ascii=False), datetime.now().isoformat(), wallet_address)
                    )
                    
                    logger.info(f"æ›´æ–° AI æ ‡ç­¾: {wallet_address}, æ ‡ç­¾: {ai_tags}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
    
    def _extract_ai_tags(self, results: Dict[str, Any]) -> List[str]:
        """ä»åˆ†æç»“æœä¸­æå– AI æ ‡ç­¾"""
        tags = []
        
        try:
            # ä»äº¤æ˜“é£æ ¼æå–
            if 'style' in results and 'style' in results['style']:
                tags.append(f"AI:{results['style']['style']}")
            
            # ä»ç­–ç•¥è¯†åˆ«æå–
            if 'strategy' in results and 'primary_strategy' in results['strategy']:
                tags.append(f"AI:{results['strategy']['primary_strategy']}")
            
            # ä»é£é™©è¯„ä¼°æå–
            if 'risk' in results and 'risk_level' in results['risk']:
                tags.append(f"AI:é£é™©{results['risk']['risk_level']}")
            
        except Exception as e:
            logger.error(f"æå– AI æ ‡ç­¾å¤±è´¥: {e}")
        
        return tags
    
    def get_cached_analysis(
        self,
        wallet_address: str,
        analysis_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        try:
            if analysis_type:
                # è·å–ç‰¹å®šç±»å‹çš„åˆ†æ
                cache = db.fetch_one("""
                    SELECT * FROM ai_analysis_cache
                    WHERE wallet_address = ? AND analysis_type = ?
                    AND expires_at > ?
                """, (wallet_address, analysis_type, datetime.now().isoformat()))
                
                if cache:
                    return {
                        'analysis_type': analysis_type,
                        'result': json.loads(cache['result']),
                        'created_at': cache['created_at'],
                        'expires_at': cache['expires_at']
                    }
                
                return None
            
            else:
                # è·å–æ‰€æœ‰ç±»å‹çš„åˆ†æ
                caches = db.fetch_all("""
                    SELECT * FROM ai_analysis_cache
                    WHERE wallet_address = ? AND expires_at > ?
                """, (wallet_address, datetime.now().isoformat()))
                
                if not caches:
                    return None
                
                results = {}
                for cache in caches:
                    results[cache['analysis_type']] = {
                        'result': json.loads(cache['result']),
                        'created_at': cache['created_at'],
                        'expires_at': cache['expires_at']
                    }
                
                return results
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜åˆ†æå¤±è´¥: {e}")
            return None
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        return {
            'running': self.running,
            'queue_size': self.queue.qsize(),
            'current_task': {
                'wallet_address': self.current_task.wallet_address,
                'analysis_types': self.current_task.analysis_types,
                'priority': self.current_task.priority.name
            } if self.current_task else None,
            'completed_tasks': len(self.task_history),
            'recent_tasks': self.task_history[-10:] if self.task_history else []
        }


# å…¨å±€ AI è°ƒåº¦å™¨å®ä¾‹
ai_scheduler = AIScheduler()


# å¯¼å‡º
__all__ = ['AIScheduler', 'ai_scheduler', 'Priority']

